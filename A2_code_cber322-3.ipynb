{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a8d2939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 3)\n",
      "(4000, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64274\\AppData\\Local\\Temp/ipykernel_21140/3763925529.py:25: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_training['ABSTRACT'] = df_training['ABSTRACT'].str.replace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Naive Bayes:\n",
      "TP + TN: 3535\n",
      "FP + FN: 465\n",
      "Accuracy: 0.88375\n"
     ]
    }
   ],
   "source": [
    "# Standard Naive Bayes Classifier:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('trg.csv', sep=',',)\n",
    "df_train = df_train.rename(columns={'id': 'ID', 'class': 'CLASS','abstract': 'ABSTRACT'})\n",
    "\n",
    "print(df_train.shape)\n",
    "df_train.head()\n",
    "\n",
    "df_train['CLASS'].value_counts(normalize=True)\n",
    "\n",
    "\n",
    "# Split data set.\n",
    "df_training = df_train\n",
    "\n",
    "print(df_training.shape)\n",
    "\n",
    "\n",
    "# cleaning\n",
    "df_training['CLASS'].value_counts(normalize=True)\n",
    "\n",
    "df_training.head(3)\n",
    "\n",
    "df_training['ABSTRACT'] = df_training['ABSTRACT'].str.replace(\n",
    "   '\\W', ' ') \n",
    "df_training['ABSTRACT'] = df_training['ABSTRACT'].str.lower()\n",
    "df_training.head(3)\n",
    "\n",
    "df_training['ABSTRACT'] = df_training['ABSTRACT'].str.split()\n",
    "voca_list = []\n",
    "for line in df_training['ABSTRACT']:\n",
    "    for string in line:\n",
    "        voca_list.append(string)\n",
    "\n",
    "voca_list = list(set(voca_list))\n",
    "len(voca_list)\n",
    "string_counts = {string: [0] * len(df_training['ABSTRACT']) for string in voca_list}\n",
    "\n",
    "for i, msg in enumerate(df_training['ABSTRACT']):\n",
    "    for string in msg:\n",
    "        string_counts[string][i] += 1\n",
    "string_counts = pd.DataFrame(string_counts)\n",
    "string_counts.head()\n",
    "df_training_clean = pd.concat([df_training, string_counts], axis=1)\n",
    "df_training.head()\n",
    "\n",
    "# setting estimates\n",
    "e_CLASS = df_training_clean[df_training_clean['CLASS'] == 'E']\n",
    "b_CLASS = df_training_clean[df_training_clean['CLASS'] == 'B']\n",
    "a_CLASS = df_training_clean[df_training_clean['CLASS'] == 'A']\n",
    "v_CLASS = df_training_clean[df_training_clean['CLASS'] == 'V']\n",
    "\n",
    "\n",
    "p_e_CLASS = len(e_CLASS) / len(df_training_clean)\n",
    "p_b_CLASS= len(b_CLASS) / len(df_training_clean)\n",
    "p_a_CLASS = len(a_CLASS) / len(df_training_clean)\n",
    "p_v_CLASS = len(v_CLASS) / len(df_training_clean)\n",
    "\n",
    "words_in_e_CLASS = e_CLASS['CLASS'].apply(len)\n",
    "n_e = words_in_e_CLASS.sum()\n",
    "\n",
    "words_in_b_CLASS = b_CLASS['CLASS'].apply(len)\n",
    "n_b = words_in_b_CLASS.sum()\n",
    "\n",
    "words_in_a_CLASS = a_CLASS['CLASS'].apply(len)\n",
    "n_a = words_in_a_CLASS.sum()\n",
    "\n",
    "words_in_v_CLASS = v_CLASS['CLASS'].apply(len)\n",
    "n_v = words_in_v_CLASS.sum()\n",
    "\n",
    "n_voca = len(voca_list)\n",
    "\n",
    "Laplace_Smoothing = 1\n",
    "\n",
    "estimates_e = {word:0 for word in voca_list}\n",
    "estimates_b = {word:0 for word in voca_list}\n",
    "estimates_a = {word:0 for word in voca_list}\n",
    "estimates_v = {word:0 for word in voca_list}\n",
    "\n",
    "# Calculate estimates\n",
    "for string in voca_list:\n",
    "    words_given_e = e_CLASS[string].sum() \n",
    "    p_word_given_e = (words_given_e + Laplace_Smoothing) / (n_e + Laplace_Smoothing*n_voca)\n",
    "    estimates_e[string] = p_word_given_e\n",
    "\n",
    "    words_given_b = b_CLASS[string].sum() \n",
    "    p_word_given_b = (words_given_b + Laplace_Smoothing) / (n_b + Laplace_Smoothing*n_voca)\n",
    "    estimates_b[string] = p_word_given_b\n",
    "    \n",
    "    words_given_a = a_CLASS[string].sum() \n",
    "    p_word_given_a = (words_given_a + Laplace_Smoothing) / (n_a + Laplace_Smoothing*n_voca)\n",
    "    estimates_a[string] = p_word_given_a\n",
    "    \n",
    "    words_given_v = v_CLASS[string].sum() \n",
    "    p_word_given_v = (words_given_v + Laplace_Smoothing) / (n_v + Laplace_Smoothing*n_voca)\n",
    "    estimates_v[string] = p_word_given_v   \n",
    "\n",
    "\n",
    "# train the model.\n",
    "import re\n",
    "import math\n",
    "def classification(abstract):\n",
    "\n",
    "    abstract = re.sub('\\W', ' ', abstract)\n",
    "    abstract = abstract.lower().split()\n",
    "\n",
    "    p_e_given_abstract = math.log(p_e_CLASS)\n",
    "    p_b_given_abstract = math.log(p_b_CLASS)\n",
    "    p_a_given_abstract = math.log(p_a_CLASS)\n",
    "    p_v_given_abstract = math.log(p_v_CLASS)\n",
    "\n",
    "    for string in abstract:\n",
    "        if string in estimates_e:\n",
    "            p_e_given_abstract += math.log(estimates_e[string])\n",
    "\n",
    "        if string in estimates_b:\n",
    "            p_b_given_abstract += math.log(estimatess_b[string])\n",
    "            \n",
    "        if string in estimates_a:\n",
    "            p_a_given_abstract += math.log(estimates_a[string])\n",
    "            \n",
    "        if string in estimates_v:\n",
    "            p_v_given_abstract += math.log(estimates_v[string])\n",
    "\n",
    "    print('P(E|abstract):', p_e_given_abstract)\n",
    "    print('P(B|abstract):', p_b_given_abstract)\n",
    "    print('P(A|abstract):', p_a_given_abstract)\n",
    "    print('P(V|abstract):', p_v_given_abstract)\n",
    "\n",
    "    if p_e_given_abstract > p_b_given_abstract and p_e_given_abstract > p_a_given_abstract and p_e_given_abstract > p_v_given_abstract:\n",
    "        print('CLASS: E')\n",
    "    elif p_b_given_abstract > p_e_given_abstract and p_b_given_abstract > p_a_given_abstract and p_b_given_abstract > p_v_given_abstract:\n",
    "        print('CLASS: B')\n",
    "    elif p_a_given_abstract > p_e_given_abstract and p_a_given_abstract > p_b_given_abstract and p_a_given_abstract > p_v_given_abstract:\n",
    "        print('CLASS: A')\n",
    "    elif p_v_given_abstract > p_e_given_abstract and p_v_given_abstract > p_b_given_abstract and p_v_given_abstract > p_a_given_abstract:\n",
    "        print('CLASS: V')\n",
    "    else:\n",
    "        print('classify')\n",
    "\n",
    "# test the model.\n",
    "\n",
    "import re\n",
    "import math\n",
    "def test_classification(abstract):\n",
    "   \n",
    "    abstract = re.sub('\\W', ' ', abstract)\n",
    "    abstract = abstract.lower().split()\n",
    "\n",
    "    p_e_given_abstract = math.log(p_e_CLASS)\n",
    "    p_b_given_abstract = math.log(p_b_CLASS)\n",
    "    p_a_given_abstract = math.log(p_a_CLASS)\n",
    "    p_v_given_abstract = math.log(p_v_CLASS)\n",
    "\n",
    "    for string in abstract:\n",
    "        if string in estimates_e:\n",
    "            p_e_given_abstract += math.log(estimates_e[string])\n",
    "\n",
    "        if string in estimates_b:\n",
    "            p_b_given_abstract += math.log(estimates_b[string])\n",
    "            \n",
    "        if string in estimates_a:\n",
    "            p_a_given_abstract += math.log(estimates_a[string])\n",
    "            \n",
    "        if string in estimates_v:\n",
    "            p_v_given_abstract += math.log(estimates_v[string])\n",
    "\n",
    "    if p_e_given_abstract > p_b_given_abstract and p_e_given_abstract > p_a_given_abstract and p_e_given_abstract > p_v_given_abstract:\n",
    "        return ('E')\n",
    "    elif p_b_given_abstract > p_e_given_abstract and p_b_given_abstract > p_a_given_abstract and p_b_given_abstract > p_v_given_abstract:\n",
    "        return ('B')\n",
    "    elif p_a_given_abstract > p_e_given_abstract and p_a_given_abstract > p_b_given_abstract and p_a_given_abstract > p_v_given_abstract:\n",
    "        return ('A')\n",
    "    elif p_v_given_abstract > p_e_given_abstract and p_v_given_abstract > p_b_given_abstract and p_v_given_abstract > p_a_given_abstract:\n",
    "        return ('V')\n",
    "    else:\n",
    "        return 'classify'\n",
    "\n",
    "\n",
    "df_test= pd.read_csv('trg.csv', sep=',')\n",
    "df_test = df_test.rename(columns={'id': 'ID', 'class': 'CLASS','abstract': 'ABSTRACT'})\n",
    "\n",
    "df_test['predicted'] = df_test['ABSTRACT'].apply(test_classification)\n",
    "df_test.head()\n",
    "\n",
    "\n",
    "count = 0\n",
    "length = df_test.shape[0]\n",
    "\n",
    "for line in df_test.iterrows():\n",
    "    line = line[1]\n",
    "    if line['CLASS'] == line['predicted']:\n",
    "        count += 1\n",
    "\n",
    "print('Standard Naive Bayes:')\n",
    "print('TP + TN:', count)\n",
    "print('FP + FN:', length - count)\n",
    "print('Accuracy:', count/length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb38e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imblearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "df_train = pd.read_csv('trg.csv', sep=',')\n",
    "df_train = df_train.rename(columns={'id': 'ID', 'class': 'CLASS','abstract': 'ABSTRACT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d87c2514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "E    2144\n",
       "B    1602\n",
       "A     128\n",
       "V     126\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['CLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d00cfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64274\\AppData\\Local\\Temp/ipykernel_21140/362778831.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_training['ABSTRACT'] = df_training['ABSTRACT'].str.replace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test number8575\n",
      "TP + TN: 3961\n",
      "FP + FN: 39\n",
      "Accuracy: 0.99025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21140/362778831.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstring\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mword_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mword_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mdf_training_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_training\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m     return arrays_to_mgr(\n\u001b[0m\u001b[0;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    587\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m             val = sanitize_array(\n\u001b[0m\u001b[0;32m    590\u001b[0m                 \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m             )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure, allow_2d)\u001b[0m\n\u001b[0;32m    569\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m             \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_convert_platform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mmaybe_convert_platform\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstruct_1d_object_array_from_listlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;31m# The caller is responsible for ensuring that we have np.ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mconstruct_1d_object_array_from_listlike\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1987\u001b[0m     \u001b[1;31m# numpy will try to interpret nested lists as further dimensions, hence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1988\u001b[0m     \u001b[1;31m# making a 1D array that contains list-likes is a bit tricky:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1989\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"object\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1990\u001b[0m     \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1991\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Improved Naive Bayes Classifier: we try the best out 10 random sampling model.\n",
    "for i in range(1,11):\n",
    "    \n",
    "    # we use oversample to balanced the data set.\n",
    "    max_size = df_train['CLASS'].value_counts().max()\n",
    "    lst = [df_train]\n",
    "    for class_index, group in df_train.groupby('CLASS'):\n",
    "        lst.append(group.sample(max_size-len(group), replace=True))\n",
    "    frame_new = pd.concat(lst)\n",
    "\n",
    "    df_training = frame_new\n",
    "    df_training['CLASS'].value_counts(normalize=True)\n",
    "    df_training.head(3)\n",
    "    \n",
    "    # we clean the data set removing punctuation and transform in lowercase.\n",
    "    df_training['ABSTRACT'] = df_training['ABSTRACT'].str.replace(\n",
    "       '\\W', ' ') \n",
    "    df_training['ABSTRACT'] = df_training['ABSTRACT'].str.lower()\n",
    "    df_training.head() \n",
    "\n",
    "    # we create a vocabulary of words excluding digits and common words for both the vocabulary and the data set.\n",
    "    text = df_training['ABSTRACT']\n",
    "    voca_list = []\n",
    "    new_train = []\n",
    "    for line in text:\n",
    "        words = [string for string in line.split() if string.lower() not in ENGLISH_STOP_WORDS and not string.isdigit()]\n",
    "        new_text = \" \".join(words)\n",
    "        new_train.append(new_text)\n",
    "        for string in new_text.split():\n",
    "            voca_list.append(string)     \n",
    "    voca_list = list(set(voca_list))\n",
    "\n",
    "    # we replace the new text into the dataset.\n",
    "    df_new_text = pd.DataFrame(new_train, columns =['ABSTRACT'])\n",
    "    df_training = df_training[['ID','CLASS']]\n",
    "    df_training['ABSTRACT'] = df_new_text\n",
    "\n",
    "    df_training['ABSTRACT'] = df_training['ABSTRACT'].str.split()\n",
    "    word_counts = {string: [0] * len(df_training['ABSTRACT']) for string in voca_list}\n",
    "\n",
    "    for i, msg in enumerate(df_training['ABSTRACT']):\n",
    "        for string in msg:\n",
    "            word_counts[string][i] += 1\n",
    "    word_counts = pd.DataFrame(word_counts)\n",
    "    df_training_clean = pd.concat([df_training.reset_index(drop=True), word_counts.reset_index(drop=True)], axis=1, sort=False)\n",
    "\n",
    "    # Group the data by CLass\n",
    "    e_CLASS = df_training_clean[df_training_clean['CLASS'] == 'E']\n",
    "    b_CLASS = df_training_clean[df_training_clean['CLASS'] == 'B']\n",
    "    a_CLASS = df_training_clean[df_training_clean['CLASS'] == 'A']\n",
    "    v_CLASS = df_training_clean[df_training_clean['CLASS'] == 'V']\n",
    "\n",
    "    # Calculating the probabilities.\n",
    "    p_e_CLASS = len(e_CLASS) / len(df_training_clean)\n",
    "    p_b_CLASS= len(b_CLASS) / len(df_training_clean)\n",
    "    p_a_CLASS = len(a_CLASS) / len(df_training_clean)\n",
    "    p_v_CLASS = len(v_CLASS) / len(df_training_clean)\n",
    "\n",
    "    # counting each class.\n",
    "    words_in_e_CLASS = e_CLASS['CLASS'].apply(len)\n",
    "    n_e = words_in_e_CLASS.sum()\n",
    "    words_in_b_CLASS = b_CLASS['CLASS'].apply(len)\n",
    "    n_b = words_in_b_CLASS.sum()\n",
    "    words_in_a_CLASS = a_CLASS['CLASS'].apply(len)\n",
    "    n_a = words_in_a_CLASS.sum()\n",
    "    words_in_v_CLASS = v_CLASS['CLASS'].apply(len)\n",
    "    n_v = words_in_v_CLASS.sum()\n",
    "\n",
    "    # number of word in the vocabulary\n",
    "    n_voca = len(voca_list)\n",
    "\n",
    "    # Laplace smoothing\n",
    "    Laplace_Smoothing = 1\n",
    "\n",
    "    # Initialize the estimates.\n",
    "    estimates_e = {word:0 for word in voca_list}\n",
    "    estimates_b = {word:0 for word in voca_list}\n",
    "    estimates_a = {word:0 for word in voca_list}\n",
    "    estimates_v = {word:0 for word in voca_list}\n",
    "\n",
    "    # Calculating estimates.\n",
    "    for string in voca_list:\n",
    "        words_given_e = e_CLASS[string].sum() \n",
    "        p_word_given_e = (words_given_e + Laplace_Smoothing) / (n_e + Laplace_Smoothing*n_voca)\n",
    "        estimates_e[string] = p_word_given_e\n",
    "\n",
    "        words_given_b = b_CLASS[string].sum() \n",
    "        p_word_given_b = (words_given_b + Laplace_Smoothing) / (n_b + Laplace_Smoothing*n_voca)\n",
    "        estimates_b[string] = p_word_given_b\n",
    "\n",
    "        words_given_a = a_CLASS[string].sum() \n",
    "        p_word_given_a = (words_given_a + Laplace_Smoothing) / (n_a + Laplace_Smoothing*n_voca)\n",
    "        estimates_a[string] = p_word_given_a\n",
    "\n",
    "        words_given_v = v_CLASS[string].sum() \n",
    "        p_word_given_v = (words_given_v + Laplace_Smoothing) / (n_v + Laplace_Smoothing*n_voca)\n",
    "        estimates_v[string] = p_word_given_v   \n",
    "    \n",
    "    # function to calculate the probabilities.\n",
    "    import re\n",
    "    import math\n",
    "    def test_classification(abstract):\n",
    "   \n",
    "        abstract = re.sub('\\W', ' ', abstract)\n",
    "        abstract = abstract.lower().split()\n",
    "\n",
    "        p_e_given_abstract = math.log(p_e_CLASS)\n",
    "        p_b_given_abstract = math.log(p_b_CLASS)\n",
    "        p_a_given_abstract = math.log(p_a_CLASS)\n",
    "        p_v_given_abstract = math.log(p_v_CLASS)\n",
    "\n",
    "        for word in abstract:\n",
    "            if word in estimates_e:\n",
    "                p_e_given_abstract += math.log(estimates_e[word])\n",
    "\n",
    "            if word in estimates_b:\n",
    "                p_b_given_abstract += math.log(estimates_b[word])\n",
    "\n",
    "            if word in estimates_a:\n",
    "                p_a_given_abstract += math.log(estimates_a[word])\n",
    "\n",
    "            if word in estimates_v:\n",
    "                p_v_given_abstract += math.log(estimates_v[word])\n",
    "\n",
    "        if p_e_given_abstract > p_b_given_abstract and p_e_given_abstract > p_a_given_abstract and p_e_given_abstract > p_v_given_abstract:\n",
    "            return ('E')\n",
    "        elif p_b_given_abstract > p_e_given_abstract and p_b_given_abstract > p_a_given_abstract and p_b_given_abstract > p_v_given_abstract:\n",
    "            return ('B')\n",
    "        elif p_a_given_abstract > p_e_given_abstract and p_a_given_abstract > p_b_given_abstract and p_a_given_abstract > p_v_given_abstract:\n",
    "            return ('A')\n",
    "        elif p_v_given_abstract > p_e_given_abstract and p_v_given_abstract > p_b_given_abstract and p_v_given_abstract > p_a_given_abstract:\n",
    "            return ('V')\n",
    "        else:\n",
    "            return 'classify'\n",
    "\n",
    "    \n",
    "    # calculating the acurracy for the training set.\n",
    "    \n",
    "    df_test= pd.read_csv('trg.csv', sep=',',)\n",
    "    df_test = df_test.rename(columns={'id': 'ID', 'class': 'CLASS','abstract': 'ABSTRACT'})\n",
    "    df_test['predicted'] = df_test['ABSTRACT'].apply(test_classification)\n",
    "\n",
    "    count = 0\n",
    "    length = df_test.shape[0]\n",
    "\n",
    "    for line in df_test.iterrows():\n",
    "        line = line[1]\n",
    "        if line['CLASS'] == line['predicted']:\n",
    "            count += 1\n",
    "        \n",
    "    print('Test number'+ str(i))\n",
    "    print('TP + TN:', count)\n",
    "    print('FP + FN:', length - count)\n",
    "    print('Accuracy:', count/length)\n",
    "    \n",
    "    # predicting values for the test set.\n",
    "    \n",
    "    df_test= pd.read_csv('tst.csv', sep=',',)\n",
    "    df_test = df_test.rename(columns={'id': 'ID', 'class': 'CLASS','abstract': 'ABSTRACT'})\n",
    "    df_test['predicted'] = df_test['ABSTRACT'].apply(test_classification)\n",
    "    filename = 'prediction' + str(i) + '.csv'\n",
    "\n",
    "    #exporting the prediction to a csv file.\n",
    "    \n",
    "    df_test.to_csv(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
